# Fine-tuning Whisper Model Step by Step

æœ¬æŒ‡å—ä»¥ `Common Voice` è³‡æ–™é›†ä½œç‚ºä¸»è¦ç¯„ä¾‹ã€‚è‹¥è¦ä½¿ç”¨æ‚¨è‡ªå·±çš„è³‡æ–™é›†ï¼ˆä¾‹å¦‚ `KsponSpeech`ï¼‰ï¼Œæ‚¨éœ€è¦é€²è¡Œä»¥ä¸‹èª¿æ•´ï¼š

1.  **æº–å‚™è³‡æ–™é›†**ï¼šç¢ºä¿æ‚¨çš„è³‡æ–™é›†ç¬¦åˆ ğŸ¤— Datasets çš„æ ¼å¼ï¼Œé€šå¸¸åŒ…å« `audio` å’Œ `sentence` å…©å€‹æ¬„ä½ã€‚æ‚¨å¯ä»¥åƒè€ƒ [kaka-lin/ksponspeech](https://github.com/kaka-lin/ksponspeech) ä¾†äº†è§£å¦‚ä½•å‰è™•ç† `KsponSpeech` è³‡æ–™é›†ã€‚

2.  **ä¿®æ”¹è¼‰å…¥è…³æœ¬**ï¼šåœ¨ `finetune.py` æˆ–é¡ä¼¼çš„è…³æœ¬ä¸­ï¼Œå°‡ `load_dataset` çš„åƒæ•¸æ›¿æ›æˆæ‚¨è‡ªå·±çš„è³‡æ–™é›†è·¯å¾‘æˆ– Hugging Face Hub ä¸Šçš„åç¨±ã€‚

## 1. Load Dataset

Common Voice æ˜¯ä¸€ç³»åˆ—ç”±ç¾¤çœ¾å¤–åŒ…ï¼ˆcrowd-sourcedï¼‰å»ºç«‹çš„è³‡æ–™é›†ï¼Œè¬›è€…æœƒåœ¨å„ç¨®èªè¨€ä¸­æœ—è®€ç¶­åŸºç™¾ç§‘ä¸Šçš„æ–‡å­—ã€‚æœ¬æ–‡æ’°å¯«æ™‚ï¼Œæˆ‘å€‘æœƒä½¿ç”¨ Common Voice è³‡æ–™é›†çš„ç‰ˆæœ¬: 13.0ã€‚è‡³æ–¼ç›®æ¨™èªè¨€ï¼Œæˆ‘å€‘å°‡åœ¨è¯èªï¼ˆzh-TWï¼‰ä¸Šï¼Œå°æ¨¡å‹åšå¾®èª¿ã€‚

> æç¤ºï¼šä½ å¯ä»¥åˆ° Hugging Face Hub ä¸Šçš„ Mozilla Foundation çµ„ç¹”é é¢ï¼ŒæŸ¥çœ‹ Common Voice è³‡æ–™é›†çš„æœ€æ–°ç‰ˆæœ¬ã€‚å¾ŒçºŒç‰ˆæœ¬æ”¯æ´æ›´å¤šèªè¨€ï¼Œä¸”æ¯ç¨®èªè¨€çš„è³‡æ–™é‡ä¹Ÿæœƒå¢åŠ ã€‚

æ¥è‘—ï¼Œæˆ‘å€‘å‰å¾€ Hubï¼Œé–‹å•Ÿ Common Voice çš„è³‡æ–™é›†é é¢ï¼š[mozilla-foundation/common_voice_13_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0)ã€‚

é¦–æ¬¡é–‹å•Ÿæ­¤é é¢æ™‚ï¼Œç³»çµ±æœƒè¦æ±‚ä½ åŒæ„ä½¿ç”¨æ¢æ¬¾ï¼›åŒæ„å¾Œå°±èƒ½å®Œæ•´å­˜å–è©²è³‡æ–™é›†ã€‚
å®Œæˆè³‡æ–™é›†ä½¿ç”¨æˆæ¬Šä¹‹å¾Œï¼Œç³»çµ±æœƒé¡¯ç¤ºè³‡æ–™é›†é è¦½ï¼ˆpreviewï¼‰ï¼Œé è¦½ä¸­åˆ—å‡ºäº†å‰ 100 ç­†ç¯„ä¾‹ï¼Œä¸¦ä¸”å…§å»ºéŸ³æª”æ’­æ”¾å™¨ï¼Œå¯ç«‹å³æ”¶è½ã€‚

ä½¿ç”¨ [ğŸ¤— Datasets](https://huggingface.co/docs/datasets/index) ä¸‹è¼‰ä¸¦æº–å‚™è³‡æ–™è®Šå¾—éå¸¸ç°¡å–®ã€‚æˆ‘å€‘åªéœ€ä¸€è¡Œç¨‹å¼ç¢¼ï¼Œå°±èƒ½ä¸‹è¼‰ä¸¦æº–å‚™ Common Voice çš„å„å€‹è³‡æ–™æ‹†åˆ†ã€‚

```python
from datasets import load_dataset, DatasetDict

common_voice = DatasetDict()

common_voice["train"] = load_dataset("mozilla-foundation/common_voice_13_0", "zh-TW", split="train+validation")
common_voice["test"] = load_dataset("mozilla-foundation/common_voice_13_0", "zh-TW", split="test"),

common_voice = common_voice.select_columns(["audio", "sentence"])
```

## 2. Feature Extractor, Tokenizer and Processor

The ASR pipeline can be de-composed into three stages:

1. ä¸€å€‹ç‰¹å¾µæ“·å–å™¨ï¼ˆfeature extractorï¼‰: è² è²¬é è™•ç†åŸå§‹éŸ³è¨Šè¼¸å…¥
2. ä¸€å€‹æ¨¡å‹: åŸ·è¡Œ sequence-to-sequence çš„æ˜ å°„
3. ä¸€å€‹æ¨™è¨˜å™¨ï¼ˆtokenizerï¼‰: å°‡æ¨¡å‹è¼¸å‡ºå¾Œè™•ç†æˆæ–‡å­—æ ¼å¼

åœ¨ğŸ¤— Transformers ä¸­ï¼ŒWhisper æ¨¡å‹æœ‰ä¸€å€‹ç›¸é—œçš„ç‰¹å¾µæå–å™¨ï¼ˆfeature extractorï¼‰å’Œæ¨™è¨˜å™¨ï¼ˆtokenizerï¼‰ï¼Œåˆ†åˆ¥ç¨±ç‚º [WhisperFeatureExtractor] å’Œ [WhisperTokenizer]ã€‚

### WhisperFeatureExtractor

Whisper feature extractorï¼ˆç‰¹å¾µæå–å™¨ï¼‰æœƒåŸ·è¡Œå…©é …æ“ä½œ:

1. å°ä¸€æ‰¹éŸ³è¨Šæ¨£æœ¬é€²è¡Œ *padding(å¡«å……)* æˆ– *truncation(æˆªæ–·)*ï¼Œä½¿æ¯ç­†æ¨£æœ¬çš„é•·åº¦éƒ½ç‚º 30 ç§’ã€‚
    - è‹¥æ¨£æœ¬çŸ­æ–¼ 30 ç§’: å‰‡åœ¨åºåˆ—æœ«ç«¯è£œé›¶ï¼ˆé›¶ä»£è¡¨éœéŸ³ï¼‰
    - è‹¥è¶…é 30 ç§’: å‰‡æˆªæ‰è¶…å‡ºéƒ¨åˆ†ã€‚

    ç”±æ–¼æ•´å€‹æ‰¹æ¬¡çš†è¢«èª¿æ•´è‡³ç›¸åŒçš„æœ€å¤§é•·åº¦ï¼Œå‚³å…¥ Whisper æ¨¡å‹æ™‚å°±ä¸éœ€è¦æä¾› *attention mask*ã€‚

    > Whisper åœ¨é€™é»ä¸Šç›¸ç•¶ç‰¹æ®Šâ”€â”€å¤§å¤šæ•¸éŸ³è¨Šæ¨¡å‹éƒ½å¿…é ˆç”¨ attention mask æ¨™ç¤ºå¡«å……ä½ç½®ï¼Œä»¥ä¾¿åœ¨è‡ªæ³¨æ„åŠ›é‹ç®—ä¸­å¿½ç•¥é‚£äº›æ™‚åˆ»ï¼Œä½† Whisper å·²ç¶“è¨“ç·´åˆ°ä¸ç”¨ attention maskï¼Œå°±èƒ½è‡ªè¡Œåˆ¤æ–·å“ªäº›éƒ¨åˆ†è©²è¢«å¿½ç•¥ã€‚

2. ç‰¹å¾µæ“·å–å™¨æœƒå°‡ padding å¾Œçš„éŸ³è¨Šé™£åˆ—è½‰æ›ç‚º[å°æ•¸æ¢…çˆ¾é »è­œåœ–ï¼ˆlog-Mel spectrogramï¼‰](https://github.com/kaka-lin/ASR-notes/blob/main/basic/audio_data/introduction.md#7-mel-spectrogram%E6%A2%85%E7%88%BE%E9%A0%BB%E8%AD%9C%E5%9C%96)ã€‚

    é€™ç¨®é »è­œåœ–é¡ä¼¼å‚…ç«‹è‘‰è½‰æ›ï¼Œå¯è¦–åŒ–åœ°å‘ˆç¾è¨Šè™Ÿåœ¨ä¸åŒé »ç‡ä¸Šçš„å¼·åº¦ã€‚ç¸±è»¸ä»£è¡¨æ¢…çˆ¾é »é“ï¼ˆå°æ‡‰ç‰¹å®šé »ç‡å€é–“ï¼‰ï¼Œæ©«è»¸ä»£è¡¨æ™‚é–“ï¼Œæ¯å€‹åƒç´ çš„é¡è‰²å‰‡é¡¯ç¤ºè©²é »ç‡å€é–“åœ¨è©²æ™‚åˆ»çš„å°æ•¸å¼·åº¦ã€‚å°æ•¸æ¢…çˆ¾é »è­œåœ–æ­£æ˜¯ Whisper æ¨¡å‹æ‰€éœ€çš„è¼¸å…¥å½¢å¼ã€‚

æ¢…çˆ¾é »é“ï¼ˆé »ç‡å€é–“ï¼‰æ˜¯èªéŸ³è™•ç†çš„æ¨™æº–è¨­ç½®ï¼Œæ—¨åœ¨æ¨¡æ“¬äººè€³çš„è½è¦ºç¯„åœã€‚åœ¨ Whisper å¾®èª¿æ™‚ï¼Œæˆ‘å€‘åªéœ€äº†è§£é »è­œåœ–æ˜¯èªéŸ³é »ç‡çš„è¦–è¦ºåŒ–å‘ˆç¾ã€‚è‹¥æƒ³æ·±å…¥ç­è§£æ¢…çˆ¾é »é“çš„åŸç†ï¼Œå¯åƒè€ƒ[ã€Œæ¢…çˆ¾é »ç‡å€’è­œã€ï¼ˆMel-frequency cepstrumï¼‰](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)ç›¸é—œæ–‡ç»ã€‚

<p align="center">
  <img src="./images/mel_spectrogram.png" alt="ç¤ºæ„åœ–" />
  <br>
  <sub>åœ–ç‰‡ä¾†æºï¼š<a href="https://research.google/blog/specaugment-a-new-data-augmentation-method-for-automatic-speech-recognition/">Google SpecAugment Blog</a></sub>
</p>

å¹¸é‹çš„æ˜¯ï¼ŒğŸ¤— Transformers Whisper feature extractor åƒ…ç”¨ä¸€è¡Œç¨‹å¼ç¢¼å°±èƒ½å®Œæˆå¡«å……å’Œé »è­œåœ–è½‰æ›ï¼è®“æˆ‘å€‘ç¹¼çºŒå¾é å…ˆè¨“ç·´å¥½çš„æª¢æŸ¥é»è¼‰å…¥ç‰¹å¾µæå–å™¨ï¼Œç‚ºæˆ‘å€‘çš„éŸ³è¨Šè³‡æ–™åšå¥½æº–å‚™ï¼š

```python
from transformers import WhisperFeatureExtractor

feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-small")
```

### WhisperTokenizer

Whisper æ¨¡å‹è¼¸å‡ºçš„ text tokenï¼ˆæ–‡å­—æ¨™è¨˜ï¼‰ä»£è¡¨é æ¸¬æ–‡å­—åœ¨è©å½™å­—å…¸ä¸­çš„ç´¢å¼•ã€‚*tokenizer* è² è²¬å°‡ä¸€é€£ä¸²çš„ text token æ˜ å°„ç‚ºå¯¦éš›çš„æ–‡å­—ä¸²ï¼Œä¾‹å¦‚ï¼š

    [1169, 3797, 3332] â†’ the cat sat

å‚³çµ±ä¸Šï¼Œç•¶ä½¿ç”¨åƒ…ç·¨ç¢¼å™¨ï¼ˆencoder-onlyï¼‰æ¶æ§‹çš„ ASR æ¨¡å‹æ™‚ï¼Œæˆ‘å€‘æœƒæ¡ç”¨ [Connectionist Temporal Classification (CTC)](https://distill.pub/2017/ctc/) é€²è¡Œè§£ç¢¼ï¼Œæ­¤æ™‚éœ€è¦ç‚ºæ¯å€‹è³‡æ–™é›†è¨“ç·´å°æ‡‰çš„ CTC tokenizerã€‚æ¡ç”¨ç·¨ç¢¼å™¨â€“è§£ç¢¼å™¨ï¼ˆencoder-decoderï¼‰æ¶æ§‹çš„ä¸€å¤§å„ªå‹¢æ˜¯ï¼Œæˆ‘å€‘å¯ä»¥ç›´æ¥ä½¿ç”¨é è¨“ç·´æ¨¡å‹æ‰€é™„å¸¶çš„ tokenizerã€‚

Whisper çš„ tokenizer å·²åœ¨ 96 ç¨®é è¨“ç·´èªè¨€çš„è½‰éŒ„è³‡æ–™ä¸Šé€²è¡Œé è¨“ç·´ï¼Œå› æ­¤å…·å‚™è±å¯Œçš„ [byte-pair](https://huggingface.co/learn/llm-course/chapter6/5?fw=pt#bytepair-encoding-tokenization)ï¼Œé©ç”¨æ–¼å¹¾ä¹æ‰€æœ‰çš„å¤šèª ASR æ‡‰ç”¨ã€‚å°æ–¼è¯èªï¼ˆzh-TWï¼‰ï¼Œæˆ‘å€‘å¯ä»¥ç›´æ¥è¼‰å…¥è©² tokenizerï¼Œä¸¦åœ¨å¾®èª¿æ™‚ç„¡éœ€ä»»ä½•é¡å¤–ä¿®æ”¹ã€‚æˆ‘å€‘åªè¦æŒ‡å®šç›®æ¨™èªè¨€å’Œä»»å‹™ï¼Œtokenizer å°±æœƒåœ¨ç·¨ç¢¼å¾Œçš„ label sequences é–‹é ­è‡ªå‹•åŠ ä¸Šå°æ‡‰çš„èªè¨€èˆ‡ä»»å‹™æ¨™è¨˜ï¼š

```python
from transformers import WhisperTokenizer

tokenizer = WhisperTokenizer.from_pretrained("openai/whisper-small", language="zh", task="transcribe")
```

é©—è­‰ï¼š

```python
input_str = common_voice["train"][0]["sentence"]
labels = tokenizer(input_str).input_ids
decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)
decoded_str = tokenizer.decode(labels, skip_special_tokens=True)

print(f"Input:                 {input_str}")
print(f"Decoded w/ special:    {decoded_with_special}")
print(f"Decoded w/out special: {decoded_str}")
print(f"Are equal:             {input_str == decoded_str}")
```

Print Output:

```bash
Input:                 åœ°åœ–ç‚®
Decoded w/ special:    <|startoftranscript|><|zh|><|transcribe|><|notimestamps|>åœ°åœ–ç‚®<|endoftext|>
Decoded w/out special: åœ°åœ–ç‚®
Are equal:             True
```

### Combine To Create A WhisperProcessor

ç‚ºäº†ç°¡åŒ–ç‰¹å¾µæå–å™¨å’Œæ¨™è¨˜å™¨çš„ä½¿ç”¨ï¼Œæˆ‘å€‘å¯ä»¥å°‡å®ƒå€‘åŒ…è£åˆ°ä¸€å€‹ [WhisperProcessor](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperProcessor) é¡åˆ¥ä¸­ã€‚æ­¤è™•ç†å™¨ç‰©ä»¶ç¹¼æ‰¿è‡ª `WhisperFeatureExtractor` å’Œ `WhisperTokenizer`ï¼Œå¯æ ¹æ“šéœ€è¦ç”¨æ–¼éŸ³è¨Šé è™•ç†å’Œæ–‡å­—æ¨™è¨˜å¾Œè™•ç†ã€‚é€™æ¨£ï¼Œæˆ‘å€‘åœ¨è¨“ç·´æœŸé–“åªéœ€è¦è¿½è¹¤å…©å€‹ç‰©ä»¶ï¼š`è™•ç†å™¨ (processor)` å’Œ `æ¨¡å‹ (model)`ï¼š

```python
from transformers import WhisperProcessor

processor = WhisperProcessor.from_pretrained("openai/whisper-small", language="zh", task="transcribe")
```

## 3. Pre-Process the Data

æˆ‘å€‘å°‡éŸ³è¨Šçš„å–æ¨£ç‡èˆ‡ Whisper æ¨¡å‹çš„å–æ¨£ç‡ï¼ˆ16kHzï¼‰ç›¸åŒ¹é…ã€‚ç”±æ–¼æˆ‘å€‘çš„è¼¸å…¥éŸ³è¨Šå–æ¨£ç‡ç‚º 48kHzï¼Œå› æ­¤æˆ‘å€‘éœ€è¦å°‡å…¶ä¸‹å–æ¨£è‡³ 16kHzï¼Œç„¶å¾Œå†å°‡å…¶å‚³éçµ¦ Whisper ç‰¹å¾µæ“·å–å™¨ã€‚

æˆ‘å€‘å°‡ä½¿ç”¨è³‡æ–™é›†çš„ [cast_column](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.cast_column) æ–¹æ³•å°‡éŸ³è¨Šè¼¸å…¥è¨­å®šç‚ºæ­£ç¢ºçš„å–æ¨£ç‡ã€‚æ­¤æ“ä½œä¸æœƒç›´æ¥æ›´æ”¹éŸ³é »ï¼Œè€Œæ˜¯åœ¨é¦–æ¬¡åŠ è¼‰éŸ³é »æ¨£æœ¬æ™‚å‘è³‡æ–™é›†ç™¼å‡ºä¿¡è™Ÿï¼Œä½¿å…¶å‹•æ…‹åœ°é‡æ–°æ¡æ¨£éŸ³é »æ¨£æœ¬ï¼š

```python
from datasets import Audio

common_voice = common_voice.cast_column("audio", Audio(sampling_rate=16000))
```

ç¾åœ¨æˆ‘å€‘å¯ä»¥æ’°å¯«ä¸€å€‹å‡½å¼ï¼Œå°‡è³‡æ–™æº–å‚™å¥½ä¾›æ¨¡å‹ä½¿ç”¨ï¼š

1. å‘¼å« `batch["audio"]` ä¾†è¼‰å…¥ä¸¦é‡æ¡æ¨£éŸ³è¨Šè³‡æ–™ã€‚å¦‚å‰æ‰€è¿°ï¼ŒğŸ¤— Datasets æœƒåœ¨åŸ·è¡Œæ™‚è‡ªå‹•å®Œæˆå¿…è¦çš„é‡æ¡æ¨£ã€‚

2. ä½¿ç”¨ç‰¹å¾µæ“·å–å™¨ï¼ˆfeature extractorï¼‰ï¼Œå¾æˆ‘å€‘çš„ä¸€ç¶­ audio array è¨ˆç®—å‡º log-Mel é »è­œåœ–çš„è¼¸å…¥ç‰¹å¾µã€‚

3. åˆ©ç”¨ tokenizer å°‡è½‰éŒ„æ–‡å­—ç·¨ç¢¼æˆå°æ‡‰çš„ label idsã€‚

```python
def prepare_dataset(batch):
    # load and resample audio data from 48 to 16kHz
    audio = batch["audio"]

    # compute log-Mel input features from input audio array
    batch["input_features"] = feature_extractor(audio["array"], sampling_rate=audio["sampling_rate"]).input_features[0]

    # encode target text to label ids
    batch["labels"] = tokenizer(batch["sentence"]).input_ids
    return batch
```

æˆ‘å€‘å¯ä»¥ä½¿ç”¨ ğŸ¤— Datasets çš„ `.map` æ–¹æ³•å°‡è³‡æ–™æº–å‚™å‡½æ•¸æ‡‰ç”¨æ–¼æ‰€æœ‰è¨“ç·´æ¨£æœ¬ï¼š

```python
common_voice = common_voice.map(
    prepare_dataset, remove_columns=common_voice.column_names["train"], num_proc=1)
```

æœ€å¾Œï¼Œæˆ‘å€‘æœƒéæ¿¾æ‰æ‰€æœ‰éŸ³è¨Šæ¨£æœ¬é•·åº¦è¶…é 30 ç§’çš„è¨“ç·´è³‡æ–™ã€‚å¦å‰‡ï¼Œé€™äº›æ¨£æœ¬æœƒè¢« Whisper ç‰¹å¾µæå–å™¨æˆªæ–·ï¼Œå¾è€Œå½±éŸ¿è¨“ç·´çš„ç©©å®šæ€§ã€‚æˆ‘å€‘å®šç¾©äº†ä¸€å€‹å‡½æ•¸ï¼Œå°å°‘æ–¼ 30 ç§’çš„æ¨£æœ¬å‚³å› Trueï¼Œå°è¶…é 30 ç§’çš„æ¨£æœ¬å‚³å› Falseï¼š

```python
max_input_length = 30.0


def is_audio_in_length_range(length):
    return length < max_input_length
```

æˆ‘å€‘é€é ğŸ¤— Datasets çš„ `.filter` æ–¹æ³•å°‡éæ¿¾å‡½æ•¸æ‡‰ç”¨æ–¼è¨“ç·´è³‡æ–™é›†çš„æ‰€æœ‰æ¨£æœ¬ï¼š

```python
common_voice["train"] = common_voice["train"].filter(
    is_audio_in_length_range,
    input_columns=["input_length"],
)
```

> æ³¨æ„ï¼šç›®å‰è³‡æ–™é›†ä½¿ç”¨ torchaudio å’Œ librosa é€²è¡ŒéŸ³è¨Šè¼‰å…¥å’Œé‡æ¡æ¨£ã€‚å¦‚æœæ‚¨å¸Œæœ›å¯¦ä½œè‡ªè¨‚è³‡æ–™è¼‰å…¥/å–æ¨£ï¼Œå¯ä»¥ä½¿ç”¨ã€Œpathã€æ¬„ä½å–å¾—éŸ³è¨Šæª”æ¡ˆè·¯å¾‘ï¼Œä¸¦å¿½ç•¥ã€Œaudioã€æ¬„ä½ã€‚

## 4. Training and Evaluation

ç¾åœ¨æˆ‘å€‘å·²ç¶“æº–å‚™å¥½è³‡æ–™ï¼Œå°±å¯ä»¥é€²å…¥è¨“ç·´æµç¨‹ã€‚[ğŸ¤— Trainer](https://huggingface.co/docs/transformers/main/main_classes/trainer) æœƒç‚ºæˆ‘å€‘å®Œæˆå¤§éƒ¨åˆ†ç¹é‡çš„å·¥ä½œï¼Œæˆ‘å€‘åªéœ€è¦åšä»¥ä¸‹å¹¾ä»¶äº‹ï¼š

- å®šç¾© data collatorï¼šdata collator æœƒå°‡é è™•ç†å¾Œçš„è³‡æ–™æ•´ç†ä¸¦è½‰æˆ PyTorch å¼µé‡ï¼Œä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚

- è©•ä¼°æŒ‡æ¨™ï¼šåœ¨è©•ä¼°éšæ®µï¼Œæˆ‘å€‘è¦ä½¿ç”¨ [word error rate (WER)](https://huggingface.co/spaces/evaluate-metric/wer) ä½œç‚ºè¡¡é‡æ¨™æº–ï¼Œéœ€è¦å®šç¾©ä¸€å€‹ `compute_metrics` å‡½å¼ä¾†é€²è¡Œæ­¤è¨ˆç®—ã€‚

- è¼‰å…¥é è¨“ç·´ checkpointï¼šéœ€è¦è¼‰å…¥é è¨“ç·´å¥½çš„æª¢æŸ¥é»ï¼Œä¸¦æ­£ç¢ºé…ç½®ä»¥ä¾›è¨“ç·´ä½¿ç”¨ã€‚

- å®šç¾©è¨“ç·´åƒæ•¸ï¼šé€™äº›åƒæ•¸å°‡ç”± ğŸ¤— Trainer åœ¨å»ºç«‹è¨“ç·´æ’ç¨‹æ™‚ä½¿ç”¨ã€‚

ç•¶æˆ‘å€‘å®Œæˆæ¨¡å‹å¾®èª¿å¾Œï¼Œå°‡åœ¨æ¸¬è©¦è³‡æ–™ä¸Šé€²è¡Œè©•ä¼°ï¼Œä»¥é©—è­‰æ¨¡å‹æ˜¯å¦å·²æ­£ç¢ºå­¸ç¿’å°‡è¯èªï¼ˆzh-TWï¼‰èªéŸ³è½‰éŒ„æˆæ–‡å­—ã€‚

### Define a Data Collator

å°æ–¼ *sequence-to-sequence* èªéŸ³æ¨¡å‹è€Œè¨€ï¼Œdata collatorï¼ˆè³‡æ–™æ•´ç†å™¨ï¼‰ç›¸ç•¶ç¨ç‰¹ï¼Œå› ç‚ºå®ƒæœƒåˆ†é–‹è™•ç†`input_features` å’Œ `labels`ï¼š

- `input_features` ç”±ç‰¹å¾µæ“·å–å™¨ï¼ˆfeature extractorï¼‰è² è²¬ã€‚
- `labels` å‰‡ç”± tokenizer è™•ç†ã€‚

`input_features` å·²ç¶“è¢«å¡«å……ï¼ˆpaddedï¼‰æˆ–æˆªæ–·è‡³ 30 ç§’ï¼Œä¸¦è½‰æ›æˆå›ºå®šç¶­åº¦çš„å°æ•¸æ¢…çˆ¾é »è­œåœ– (log-Mel spectrogram)ï¼Œå› æ­¤æˆ‘å€‘åªéœ€è¦å°‡å®ƒå€‘è½‰ç‚ºæ‰¹æ¬¡ï¼ˆbatchedï¼‰çš„ PyTorch å¼µé‡ã€‚é€™å¯ä»¥é€éç‰¹å¾µæ“·å–å™¨çš„ `.pad` æ–¹æ³•ä¸¦è¨­å®š `return_tensors="pt"` ä¾†å®Œæˆã€‚ç”±æ–¼è¼¸å…¥ç¶­åº¦å·²å›ºå®šï¼Œæ­¤è™•ä¸æœƒå†é¡å¤–å¡«å……ï¼›åƒ…å°‡ `input_features` è½‰ç‚º PyTorch å¼µé‡å³å¯ã€‚

å¦ä¸€æ–¹é¢ï¼Œ`labels` ä¸¦æœªäº‹å…ˆå¡«å……ã€‚æˆ‘å€‘å…ˆä½¿ç”¨ tokenizer çš„ `.pad` æ–¹æ³•ï¼Œå°‡æ‰€æœ‰åºåˆ—å¡«å……åˆ°è©²æ‰¹æ¬¡ä¸­çš„æœ€å¤§é•·åº¦ï¼›æ¥è‘—ï¼Œå°‡å¡«å……æ¨™è¨˜æ›¿æ›æˆ `-100`ï¼Œä»¥ç¢ºä¿é€™äº›ä½ç½®åœ¨è¨ˆç®—æå¤±ï¼ˆlossï¼‰æ™‚è¢«å¿½ç•¥ã€‚æœ€å¾Œï¼Œæˆ‘å€‘æœƒå¾æ¨™ç±¤åºåˆ—é–‹é ­å»æ‰ã€Œè½‰éŒ„èµ·å§‹ã€æ¨™è¨˜ï¼Œå› ç‚ºåœ¨è¨“ç·´æ™‚æœƒç¨å¾Œå†å°‡å®ƒåŠ å›ä¾†ã€‚

æˆ‘å€‘å¯ä»¥åˆ©ç”¨å…ˆå‰å®šç¾©çš„ WhisperProcessorï¼ŒåŒæ™‚åŸ·è¡Œç‰¹å¾µæ“·å–å™¨èˆ‡ tokenizer çš„ç›¸é—œæ“ä½œï¼š

```python
import torch

from dataclasses import dataclass
from typing import Any, Dict, List, Union


@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        # split inputs and labels since they have to be of different lengths and need different padding methods
        # first treat the audio inputs by simply returning torch tensors
        input_features = [
            {"input_features": feature["input_features"][0]} for feature in features
        ]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # get the tokenized label sequences
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        # pad the labels to max length
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )

        # if bos token is appended in previous tokenization step,
        # cut bos token here as it's append later anyways
        # bos: beginning of transcript token
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels

        return batch
```
ç¾åœ¨æˆ‘å€‘å¯ä»¥åˆå§‹åŒ–å‰›å‰›å®šç¾©çš„è³‡æ–™æ•´ç†å™¨ï¼š
```python
data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
```

### Evaluation Metrics

æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘å®šç¾©åœ¨è©•ä¼°é›†ä¸Šæ‰€ä½¿ç”¨çš„è©•ä¼°æŒ‡æ¨™ã€‚æˆ‘å€‘å°‡ä½¿ç”¨ [Word Error Rate (WER)](https://huggingface.co/learn/audio-course/chapter5/evaluation) æŒ‡æ¨™ï¼Œé€™æ˜¯è©•ä¼° ASR ç³»çµ±çš„ã€Œå¯¦éš›ã€æŒ‡æ¨™ã€‚

æˆ‘å€‘å°‡å¾ ğŸ¤— Evaluate è¼‰å…¥ WER æŒ‡æ¨™ï¼š

```python
import evaluate

metric = evaluate.load("wer")
```

ç„¶å¾Œï¼Œæˆ‘å€‘åªéœ€å®šç¾©ä¸€å€‹å‡½æ•¸ï¼Œè©²å‡½æ•¸æ¥æ”¶æ¨¡å‹é æ¸¬ä¸¦å‚³å› WER æŒ‡æ¨™ã€‚é€™å€‹åç‚º `compute_metrics` çš„å‡½æ•¸é¦–å…ˆå°‡ `label_ids` ä¸­çš„ `pad_token_id` æ›¿æ›ç‚º `-100`ï¼ˆæ’¤éŠ·äº†æˆ‘å€‘åœ¨è³‡æ–™æ•´ç†å™¨ä¸­æ‡‰ç”¨çš„æ­¥é©Ÿï¼Œä»¥ä¾¿åœ¨æå¤±å‡½æ•¸ä¸­æ­£ç¢ºå¿½ç•¥å¡«å……çš„æ¨™è¨˜ï¼‰ã€‚ç„¶å¾Œï¼Œå®ƒå°‡é æ¸¬å€¼å’Œæ¨™ç±¤ ID è§£ç¢¼ç‚ºå­—ä¸²ã€‚æœ€å¾Œï¼Œè¨ˆç®—é æ¸¬å€¼å’Œåƒè€ƒæ¨™ç±¤ä¹‹é–“çš„ WERã€‚åœ¨é€™è£¡ï¼Œæˆ‘å€‘å¯ä»¥é¸æ“‡ä½¿ç”¨ *normalised (æ¨™æº–åŒ–)* çš„è½‰éŒ„å’Œé æ¸¬å€¼ï¼ˆå·²åˆªé™¤æ¨™é»ç¬¦è™Ÿå’Œå¤§å°å¯«ï¼‰é€²è¡Œè©•ä¼°ã€‚æˆ‘å€‘å»ºè­°æ‚¨éµå¾ªæ­¤æ–¹æ³•ï¼Œä»¥ä¾¿å¾æ¨™æº–åŒ–è½‰éŒ„å€¼ç²å¾—çš„ WER æ”¹é€²ä¸­å—ç›Šã€‚

```python
from transformers.models.whisper.english_normalizer import BasicTextNormalizer

normalizer = BasicTextNormalizer()


def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # replace -100 with the pad_token_id
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

    # we do not want to group tokens when computing the metrics
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)

    # compute orthographic wer
    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)

    # compute normalised WER
    pred_str_norm = [normalizer(pred) for pred in pred_str]
    label_str_norm = [normalizer(label) for label in label_str]
    # filtering step to only evaluate the samples that correspond to non-zero references:
    pred_str_norm = [
        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0
    ]
    label_str_norm = [
        label_str_norm[i]
        for i in range(len(label_str_norm))
        if len(label_str_norm[i]) > 0
    ]

    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)

    return {"wer_ortho": wer_ortho, "wer": wer}
```

### Load a Pre-Trained Checkpoint

ç¾åœ¨è®“æˆ‘å€‘è¼‰å…¥é å…ˆè¨“ç·´å¥½çš„ Whisper small checkpointã€‚åŒæ¨£ï¼Œé€éä½¿ç”¨ ğŸ¤— Transformersï¼Œé€™å¾ˆå®¹æ˜“åšåˆ°ï¼

```python
from transformers import WhisperForConditionalGeneration

model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
```

åœ¨è¨“ç·´éç¨‹ä¸­æˆ‘å€‘å°‡ `use_cache` è¨­ç‚º `False`ï¼Œå› ç‚ºæˆ‘å€‘ä½¿ç”¨äº† [æ¢¯åº¦æª¢æŸ¥é»ï¼ˆgradient checkpointingï¼‰](https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-checkpointing)ï¼Œå…©è€…ä¸ç›¸å®¹ã€‚åŒæ™‚ï¼Œæˆ‘å€‘æœƒè¦†è“‹å…©å€‹ç”Ÿæˆï¼ˆgenerationï¼‰åƒæ•¸ï¼Œä»¥æ§åˆ¶æ¨¡å‹åœ¨æ¨ç†éç¨‹ä¸­çš„è¡Œç‚ºï¼šé€éè¨­å®š`èªè¨€(language)`å’Œ`ä»»å‹™(task)`åƒæ•¸ï¼Œå¼·åˆ¶æ¨¡å‹åœ¨ç”Ÿæˆæ™‚åŠ ä¸Šèªè¨€èˆ‡ä»»å‹™æ¨™è¨˜ï¼›ä¸¦åœ¨æ¨è«–æ™‚é‡æ–°å•Ÿç”¨ cacheï¼Œä»¥åŠ å¿«ç”Ÿæˆé€Ÿåº¦ï¼š

```python
from functools import partial

# disable cache during training since it's incompatible with gradient checkpointing
model.config.use_cache = False

# set language and task for generation and re-enable cache
model.generate = partial(
    model.generate, language="zh", task="transcribe", use_cache=True
)
```

### Define the Training Configuration

åœ¨æœ€å¾Œä¸€æ­¥ä¸­ï¼Œæˆ‘å€‘å®šç¾©æ‰€æœ‰èˆ‡è¨“ç·´ç›¸é—œçš„åƒæ•¸ã€‚åœ¨é€™è£¡ï¼Œæˆ‘å€‘å°‡è¨“ç·´æ­¥æ•¸è¨­å®šç‚º 500ã€‚èˆ‡é å…ˆè¨“ç·´çš„ Whisper æ¨¡å‹ç›¸æ¯”ï¼Œé€™äº›æ­¥æ•¸è¶³ä»¥ä½¿å…¶å­—éŒ¯èª¤ç‡ (WER) é¡¯è‘—æå‡ã€‚æœ‰é—œè¨“ç·´åƒæ•¸çš„æ›´å¤šè©³ç´°ä¿¡æ¯ï¼Œè«‹åƒé–± [Seq2SeqTrainingArguments æ–‡ä»¶](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)ã€‚

ä»¥ä¸‹èªªæ˜å…¶ä¸­éƒ¨åˆ†åƒæ•¸ï¼š

- `output_dir`: ç”¨ä¾†å„²å­˜æ¨¡å‹æ¬Šé‡çš„ local ç›®éŒ„ï¼ŒåŒæ™‚ä¹Ÿæœƒæˆç‚º Hugging Face Hub ä¸Šçš„å„²å­˜åº«åç¨±ã€‚
- `generation_max_length`: åœ¨è©•ä¼°æ™‚ï¼Œè‡ªå›æ­¸ç”Ÿæˆéšæ®µå…è¨±ç”¢ç”Ÿçš„æœ€å¤§ token æ•¸ã€‚
- `save_steps`: è¨“ç·´éç¨‹ä¸­ï¼Œæ¯éš”é€™éº¼å¤šæ­¥å°±æœƒå„²å­˜ä¸€æ¬¡ä¸­é–“æª¢æŸ¥é»ï¼Œä¸¦ä¸” asynchronously(éåŒæ­¥) ä¸Šå‚³åˆ° Hubã€‚
- `eval_steps`: è¨“ç·´éç¨‹ä¸­ï¼Œæ¯éš”é€™éº¼å¤šæ­¥å°±æœƒå°ä¸­é–“æª¢æŸ¥é»åŸ·è¡Œä¸€æ¬¡è©•ä¼°ã€‚
- `report_to`: æŒ‡å®šè¦å°‡è¨“ç·´æ—¥èªŒå›å ±åˆ°å“ªè£¡ã€‚æ”¯æ´çš„å¹³å°æœ‰ `"azure_ml"`, `"comet_ml"`,`"mlflow"`, `"neptune"`, `"tensorboard"` å’Œ `"wandb"`ã€‚å¯é¸æ“‡ä½ å–œæ­¡çš„ï¼Œæˆ–ç¶­æŒé è¨­çš„ ""tensorboardâ€ å°‡æ—¥èªŒåŒæ­¥åˆ° Hubã€‚

```python
from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-small-zh",  # name on the HF Hub
    per_device_train_batch_size=16,
    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size
    learning_rate=1e-5,
    lr_scheduler_type="constant_with_warmup",
    warmup_steps=50,
    max_steps=500,  # increase to 4000 if you have your own GPU or a Colab paid plan
    gradient_checkpointing=True,
    fp16=True,
    fp16_full_eval=True,
    evaluation_strategy="steps",
    per_device_eval_batch_size=16,
    predict_with_generate=True,
    generation_max_length=225,
    save_steps=500,
    eval_steps=500,
    logging_steps=25,
    report_to=["tensorboard"],
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    push_to_hub=True,
)
```
> If you do not want to upload the model checkpoints to the Hub, set `push_to_hub=False`.

We can forward the training arguments to the ğŸ¤— Trainer along with our model, dataset, data collator and `compute_metrics` function:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=common_voice["train"],
    eval_dataset=common_voice["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor,
)
```

### Traning

To launch training, simply execute:

```python
trainer.train()
```

## Reference

- [Hugging Face/Audio Course/Fine-tuning](https://huggingface.co/learn/audio-course/chapter5/fine-tuning)
- [Fine-Tune Whisper For Multilingual ASR with ğŸ¤— Transformers ](https://huggingface.co/blog/fine-tune-whisper#training-and-evaluation)
