import os
import time
import threading
import traceback
from queue import Queue
import subprocess
from collections import defaultdict
import statistics

import torch
from faster_whisper import WhisperModel

# test audio file path
AUDIO_PATH = "test_audio.wav"

# è«‹æ±‚æ•¸é‡
REQUESTS = 4

# model config
MODEL_PATH = "Systran/faster-whisper-large-v3"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
COMPUTE_TYPE = "float16" if DEVICE == "cuda" else "int8"

# æ“·å–é‚è¼¯æ ¸å¿ƒæ•¸
def get_cpu_count():
    c = os.cpu_count()
    return c if c is not None else 1

# CPU æ ¸å¿ƒæ•¸èˆ‡æ¯è«‹æ±‚æ ¸å¿ƒæ•¸
CPU_CORES = get_cpu_count()
PER_REQUEST_CORES = max(1, CPU_CORES // REQUESTS)

# å¾…é¸çš„ cpu_threads å’Œ num_workers
CPU_THREADS_LIST = [2**i for i in range(CPU_CORES.bit_length()) if 2**i < CPU_CORES]
NUM_WORKER_LIST = [2**i for i in range(CPU_CORES.bit_length()) if 2**i < CPU_CORES]

# å…¨å±€é– (Singleton æ¨¡å¼ä¸‹ä¿è­·å–®ä¸€æ¨¡å‹)
_singleton_lock = threading.Lock()


def get_gpu_memory():
    """ç›£æ¸¬ GPU è¨˜æ†¶é«”ä½¿ç”¨é‡ (GB)"""
    try:
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=memory.used", "--format=csv,noheader,nounits"],
            capture_output=True, text=True,
        )
        memory_used = result.stdout.strip().split("\n")  # å–å¾—æ‰€æœ‰ GPU çš„è¨˜æ†¶é«”ä½¿ç”¨ç‹€æ…‹
        memory_used = [float(mem) / 1024 for mem in memory_used]  # è½‰æ›æˆ GB
        return memory_used[0] if len(memory_used) > 0 else 0.0
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ç²å– GPU è¨˜æ†¶é«”ä½¿ç”¨é‡: {e}")
        return 0.0


def print_top_k_and_lowest_resource(summary, label, k=3):
    print(f"\n=== {label} - Top {k} æœ€ä½³å¹³å‡å»¶é²çµ„åˆ ===")
    sorted_summary = sorted(summary, key=lambda x: x["mean_time"])
    top_k = sorted_summary[:k]
    for entry in top_k:
        print(f" [{label}][cpu_threads={entry['cpu_threads']}, num_workers={entry['num_workers']}] "
              f"è€—æ™‚ {entry['mean_time']:.3f}s (StdDev: {entry['std_time']:.3f})")

    # è³‡æºæ¶ˆè€—æœ€å°çš„çµ„åˆ (cpu_threads + num_workers æœ€å°ï¼Œä¸”è€—æ™‚èˆ‡æœ€ä½è€—æ™‚å·® < 5%)
    min_time = top_k[0]["mean_time"]
    best_efficiency = min(
        (e for e in top_k if e["mean_time"] <= min_time * 1.05),  # å·®ç•°å°æ–¼5%
        key=lambda e: e["cpu_threads"] + e["num_workers"]
    )
    print(f"\nğŸ‘‰ å»ºè­°ä½¿ç”¨ä½è³‡æºçµ„åˆï¼š"
          f"[{label}][cpu_threads={best_efficiency['cpu_threads']}, num_workers={best_efficiency['num_workers']}] "
          f"è€—æ™‚ {best_efficiency['mean_time']:.3f}s (StdDev: {best_efficiency['std_time']:.3f})")


def download_model(model_path, download_root="whisper_models"):
    """ä¸‹è¼‰æ¨¡å‹åˆ°æŒ‡å®šç›®éŒ„"""
    try:
        model = WhisperModel(
            model_path,
            download_root=download_root,
            local_files_only=False,  # å…è¨±å¾ HuggingFace ä¸‹è¼‰
        )
        print(f"âœ… æ¨¡å‹ {model_path} ä¸‹è¼‰æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹ {model_path} ä¸‹è¼‰å¤±æ•—: {e}")


def single_request_benchmark(cpu_threads, num_workers, download_root="whisper_models"):
    """å–®è«‹æ±‚æ¸¬è©¦ï¼Œè¿”å›ç¸½è€—æ™‚"""
    try:
        model = WhisperModel(
            MODEL_PATH,
            device=DEVICE,
            compute_type=COMPUTE_TYPE,
            cpu_threads=cpu_threads,
            num_workers=num_workers,
            download_root=download_root,
            local_files_only=True,  # ç¢ºä¿ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        )
        start = time.time()
        model.transcribe(AUDIO_PATH)
        elapsed = time.time() - start
        return elapsed
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print(f"    [å–®è«‹æ±‚][cpu_threads={cpu_threads},num_workers={num_workers}] å¤±æ•—: CUDA OOM")
        else:
            print(f"    [å–®è«‹æ±‚][cpu_threads={cpu_threads},num_workers={num_workers}] å¤±æ•—: RuntimeError - {str(e)}")
        return None
    except Exception as e:
        print(f"    [å–®è«‹æ±‚][cpu_threads={cpu_threads},num_workers={num_workers}] å¤±æ•—: {e.__class__.__name__}")
        return None



#############################################
# æ–¹æ¡ˆ1 (Singleton): æ‰€æœ‰ thread å…±äº«ä¸€å€‹ model å¯¦ä¾‹
#############################################
def test_shared_model(cpu_threads, num_workers, download_root="whisper_models"):
    times = []
    lock = threading.Lock()
    oom_flag = False

    # å»ºç«‹å…±ç”¨çš„ model å¯¦ä¾‹
    shared_model = WhisperModel(
        MODEL_PATH,
        device=DEVICE,
        compute_type=COMPUTE_TYPE,
        cpu_threads=cpu_threads,
        num_workers=num_workers,
        download_root=download_root,
        local_files_only=True,  # ç¢ºä¿ä½¿ç”¨æœ¬åœ°æ¨¡å‹
    )

    def worker(room_id):
        nonlocal oom_flag
        try:
            start = time.time()
            with _singleton_lock:
                # ç¢ºä¿åªæœ‰ä¸€å€‹ thread åœ¨ä½¿ç”¨ shared_model
                shared_model.transcribe(AUDIO_PATH)
            elapsed = time.time() - start
            with lock:
                times.append(elapsed)
            print(f"    [è«‹æ±‚ {room_id}][Singleton][cpu_threads={cpu_threads},num_workers={num_workers}] è€—æ™‚ {elapsed:.3f}s")
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                oom_flag = True
                print(f"    [è«‹æ±‚ {room_id}][Singleton][cpu_threads={cpu_threads},num_workers={num_workers}] å¤±æ•—: CUDA OOM")
            else:
                print(f"    [è«‹æ±‚ {room_id}][Singleton][cpu_threads={cpu_threads},num_workers={num_workers}] RuntimeError: {e}")
        except Exception as e:
            oom_flag = True
            print(f"    [è«‹æ±‚ {room_id}][Singleton][cpu_threads={cpu_threads},num_workers={num_workers}] å¤±æ•—: {e.__class__.__name__}")

    threads = []
    for i in range(REQUESTS):
        t = threading.Thread(target=worker, args=(i,))
        t.start()
        threads.append(t)
    for t in threads:
        t.join()

    if oom_flag or len(times) < REQUESTS:
        return None, None
    total_time = sum(times)
    avg_time = total_time / len(times) if times else 0
    return total_time, avg_time


#############################################
# æ–¹æ¡ˆ2 (Non-Singleton)ï¼šæ¯å€‹ thread ç¨ç«‹å‰µå»º model å¯¦ä¾‹
#############################################
def test_individual_model(cpu_threads, num_workers, download_root="whisper_models"):
    times = []
    lock = threading.Lock()
    oom_flag = False

    def worker(room_id):
        nonlocal oom_flag
        try:
            # æ¯å€‹ thread å–®ç¨å‰µå»º model å¯¦ä¾‹
            model = WhisperModel(
                MODEL_PATH,
                device=DEVICE,
                compute_type=COMPUTE_TYPE,
                cpu_threads=cpu_threads,
                num_workers=num_workers,
                download_root=download_root,
                local_files_only=True,  # ç¢ºä¿ä½¿ç”¨æœ¬åœ°æ¨¡å‹
            )

            start = time.time()
            model.transcribe(AUDIO_PATH)
            elapsed = time.time() - start
            with lock:
                times.append(elapsed)
            print(f"    [è«‹æ±‚ {room_id}][Nonâ€Singleton][cpu_threads={cpu_threads},num_workers={num_workers}] è€—æ™‚ {elapsed:.3f}s")
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                oom_flag = True
                print(f"    [è«‹æ±‚ {room_id}][Nonâ€Singleton][cpu_threads={cpu_threads},num_workers={num_workers}] å¤±æ•—: CUDA OOM")
            else:
                print(f"    [è«‹æ±‚ {room_id}][Nonâ€Singleton][cpu_threads={cpu_threads},num_workers={num_workers}] RuntimeError: {e}")
        except Exception as e:
            oom_flag = True
            print(f"    [è«‹æ±‚ {room_id}][Nonâ€Singleton][cpu_threads={cpu_threads},num_workers={num_workers}] å¤±æ•—: {e.__class__.__name__}")

    threads = []
    for i in range(REQUESTS):
        t = threading.Thread(target=worker, args=(i,))
        t.start()
        threads.append(t)
    for t in threads:
        t.join()

    if oom_flag or len(times) < REQUESTS:
        return None, None
    total_time = sum(times)
    avg_time = total_time / len(times) if times else 0
    return total_time, avg_time


#############################################
# åŸ·è¡Œä¸åŒåƒæ•¸çµ„åˆæ¸¬è©¦
#############################################
def run_all_benchmarks(repetitions=1):
    if not os.path.isfile(AUDIO_PATH):
        print(f"âŒ æ‰¾ä¸åˆ°éŸ³è¨Šæª”ï¼š{AUDIO_PATH}")
        return None, None

    # dowmload æ¨¡å‹ first
    print(f"æ­£åœ¨ä¸‹è¼‰æ¨¡å‹ï¼š{MODEL_PATH} ...")
    download_model(MODEL_PATH)

    print(f"\n=== é–‹å§‹å…¨é¢ Benchmark ===")
    print(f"éŸ³è¨Šæª”ï¼š{AUDIO_PATH}")
    print(f"æ¨¡å‹å¤§å°ï¼š{MODEL_PATH}")
    print(f"è«‹æ±‚æ•¸é‡ï¼š{REQUESTS}ï¼ŒCPU ç¸½æ ¸å¿ƒï¼š{CPU_CORES}ï¼Œå¹³å‡æ¯è«‹æ±‚ coresï¼š{PER_REQUEST_CORES}\n")

    # é¸æ“‡è¦å˜—è©¦çš„ cpu_threads / num_workers çµ„åˆ
    # å› ç‚º cpu_thread * num_workers < CPU_CORES
    # æ‰€ä»¥æˆ‘å€‘åªé¸æ“‡ <= PER_REQUEST_CORES çš„çµ„åˆ
    cpu_threads_list = [t for t in CPU_THREADS_LIST if t <= PER_REQUEST_CORES]
    num_worker_list = [n for n in NUM_WORKER_LIST if n <= PER_REQUEST_CORES]
    print(f"å¯ç”¨çš„ cpu_threads: {cpu_threads_list}")
    print(f"å¯ç”¨çš„ num_workers: {num_worker_list}")

    # å„²å­˜å¤šæ¬¡æ¸¬è©¦çµæœ
    all_single_results = []  # æœƒæœ‰ repeats * len(cpu_threads_list)*len(num_worker_list) ç­†è³‡æ–™
    all_multi_results  = []

    for rep in range(repetitions):
        print(f"\n--- ç¬¬ {rep+1} æ¬¡é‡è¤‡æ¸¬è©¦é–‹å§‹ ---\n")

        # 1. å…ˆåšå–®è«‹æ±‚æ¸¬è©¦ï¼Œç¢ºèªæ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸é‹è¡Œ
        print(f"=== [å–®è«‹æ±‚æ¸¬è©¦] ===")
        for cpu_threads in cpu_threads_list:
            for num_workers in num_worker_list:
                elapsed = single_request_benchmark(cpu_threads, num_workers)
                if elapsed is not None:
                    print(f"  [å–®è«‹æ±‚][cpu_threads={cpu_threads}, num_workers={num_workers}] è€—æ™‚ {elapsed:.3f}s")
                else:
                    print(f"  [å–®è«‹æ±‚][cpu_threads={cpu_threads}, num_workers={num_workers}] OOM æˆ–å¤±æ•—")
                # å„²å­˜çµæœ
                all_single_results.append({
                    "mode": "single-room",
                    "cpu_threads": cpu_threads,
                    "num_workers": num_workers,
                    "avg_time": elapsed,
                    "oom": (elapsed is None)
                })

        # 2. æ ¹æ“šå–®è«‹æ±‚æ¸¬è©¦çµæœï¼Œé€²è¡Œå¤šè«‹æ±‚æ¸¬è©¦: Singleton æ¨¡å¼
        print(f"=== [å¤šè«‹æ±‚æ¸¬è©¦] Singleton ===")
        for cpu_threads in cpu_threads_list:
            for num_workers in num_worker_list:
                total_time_s, avg_time_s = test_shared_model(cpu_threads, num_workers)
                if avg_time_s is not None:
                    print(f"  [Singleton][cpu_threads={cpu_threads}, num_workers={num_workers}] ç¸½è€—æ™‚={total_time_s:.3f}s, å¹³å‡={avg_time_s:.3f}s")
                else:
                    print(f"  [Singleton][cpu_threads={cpu_threads}, num_workers={num_workers}] å¤±æ•— (OOM æˆ– å…¶ä»–éŒ¯èª¤)")
                # å„²å­˜çµæœ
                all_multi_results.append({
                    "mode": "singleton",
                    "cpu_threads": cpu_threads,
                    "num_workers": num_workers,
                    "total_time": total_time_s,
                    "avg_time": avg_time_s,
                    "oom": (avg_time_s is None)
                })

        # 3. æ ¹æ“šå–®è«‹æ±‚æ¸¬è©¦çµæœï¼Œé€²è¡Œå¤šè«‹æ±‚æ¸¬è©¦: Non-Singleton æ¨¡å¼
        print(f"=== [å¤šè«‹æ±‚æ¸¬è©¦] Non-Singleton ===")
        for cpu_threads in cpu_threads_list:
            for num_workers in num_worker_list:
                total_time_n, avg_time_n = test_individual_model(cpu_threads, num_workers)
                if avg_time_n is not None:
                    print(f"  [Nonâ€Singleton][cpu_threads={cpu_threads}, num_workers={num_workers}] ç¸½è€—æ™‚={total_time_n:.3f}s, å¹³å‡={avg_time_n:.3f}s")
                else:
                    print(f"  [Nonâ€Singleton][cpu_threads={cpu_threads}, num_workers={num_workers}] å¤±æ•— (OOM æˆ– å…¶ä»–éŒ¯èª¤)")
                # å„²å­˜çµæœ
                all_multi_results.append({
                    "mode": "non-singleton",
                    "cpu_threads": cpu_threads,
                    "num_workers": num_workers,
                    "total_time": total_time_n,
                    "avg_time": avg_time_n,
                    "oom": (avg_time_n is None)
                })

        print(f"--- ç¬¬ {rep+1} æ¬¡é‡è¤‡æ¸¬è©¦çµæŸ ---\n")

    # === çµ±è¨ˆã€Œå–®è«‹æ±‚æœ€ä½³ã€ ===
    #   å…ˆç¯©æ‰ OOMï¼Œå–å‡ºæ¯ç¨® (cpu_threads, num_workers) åœ¨ repetitions æ¬¡è£¡é¢çš„ avg_time å¹³å‡å€¼
    single_aggregate = defaultdict(list)  # key=(cpu_threads, num_workers) -> [list of avg_time]
    for r in all_single_results:
        key = (r["cpu_threads"], r["num_workers"])
        if not r["oom"] and r["avg_time"] is not None:
            single_aggregate[key].append(r["avg_time"])

    single_summary = []  # æœƒæ”¾ {"cpu_threads":..., "num_workers":..., "mean_time":..., "std":...}
    for key, times in single_aggregate.items():
        cpu_threads, num_workers = key
        mean_t = statistics.mean(times)
        std_t = statistics.pstdev(times) if len(times) > 1 else 0.0
        single_summary.append({
            "cpu_threads": cpu_threads,
            "num_workers": num_workers,
            "mean_time": mean_t,
            "std_time": std_t
        })

    if single_summary:
        best_single = min(single_summary, key=lambda x: x["mean_time"])
        print("=== å–®è«‹æ±‚ - é‡è¤‡æ¸¬è©¦å¾Œæœ€ä½³é…ç½® (Single-Request Best) ===")
        print(f" cpu_threads : {best_single['cpu_threads']}")
        print(f" num_workers : {best_single['num_workers']}")
        print(f" å¹³å‡å»¶é²    : {best_single['mean_time']:.3f}s  (StdDev: {best_single['std_time']:.3f})")

        print_top_k_and_lowest_resource(single_summary, label="å–®è«‹æ±‚", k=3)
    else:
        print("âŒ å–®è«‹æ±‚æ‰€æœ‰é…ç½®çš† OOM æˆ–å¤±æ•—ã€‚")

    # === çµ±è¨ˆã€Œå¤šè«‹æ±‚ä½µç™¼æœ€ä½³ã€ ===
    multi_aggregate = defaultdict(list)  # key=(mode, cpu_threads, num_workers)
    for r in all_multi_results:
        key = (r["mode"], r["cpu_threads"], r["num_workers"])
        if not r["oom"] and r["avg_time"] is not None:
            multi_aggregate[key].append(r["avg_time"])

    multi_summary = []
    for key, times in multi_aggregate.items():
        mode, cpu_threads, num_workers = key
        mean_t = statistics.mean(times)
        std_t = statistics.pstdev(times) if len(times) > 1 else 0.0
        multi_summary.append({
            "mode": mode,
            "cpu_threads": cpu_threads,
            "num_workers": num_workers,
            "mean_time": mean_t,
            "std_time": std_t
        })

    if multi_summary:
        best_multi = min(multi_summary, key=lambda x: x["mean_time"])
        print(f"\n=== {REQUESTS}è«‹æ±‚ä½µç™¼ - é‡è¤‡æ¸¬è©¦å¾Œæœ€ä½³é…ç½® ({REQUESTS}-Requests Concurrency Best) ===")
        print(f" æ¨¡å¼        : {best_multi['mode']}")
        print(f" cpu_threads : {best_multi['cpu_threads']}")
        print(f" num_workers : {best_multi['num_workers']}")
        print(f" å¹³å‡å»¶é²    : {best_multi['mean_time']:.3f} s  (StdDev: {best_multi['std_time']:.3f})")

        # éæ¿¾ç›¸åŒ mode
        for mode in sorted(set(r["mode"] for r in multi_summary)):
            mode_summary = [r for r in multi_summary if r["mode"] == mode]
            print_top_k_and_lowest_resource(mode_summary, label=f"{REQUESTS}è«‹æ±‚-{mode}", k=3)
    else:
        print(f"âŒ {REQUESTS}è«‹æ±‚ä½µç™¼ã„§æ‰€æœ‰é…ç½®çš† OOM æˆ–å¤±æ•—ã€‚")

    return best_single, best_multi  # å›å‚³çµ¦å¤–å±¤å‘¼å«è€…


def main():
    repetitions = 1
    run_all_benchmarks(repetitions=repetitions)


if __name__ == "__main__":
    main()
